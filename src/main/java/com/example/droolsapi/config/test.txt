openssl pkcs8 -in rsa_key.p8 -topk8 -nocrypt -out rsa_key_nopass.pem

openssl pkcs8 -in rsa_key_nopass.pem -topk8 -nocrypt -outform DER -out rsa_key.der
base64 -w0 rsa_key.der > rsa_key.der.b64


databricks fs cp rsa_key.der.b64 dbfs:/FileStore/keys/rsa_key.der.b64



with open("/dbfs/FileStore/keys/rsa_key.der.b64") as f:
    key_b64 = f.read().strip()

sfOptions = {
    "sfURL": "uhgdwaas.east-us-2.azure.snowflakecomputing.com",
    "sfUser": "BEERAVALLI_REDDY@OPTUM.COM",
    "sfRole": "AZU_SDRP_LUTF_STG_DEVELOPER_ROLE",
    "sfWarehouse": "LUTF_STG_EVISOR_WH",
    "sfDatabase": "LUTF_STG_EVISOR_DB",
    "sfSchema": "ETL",
    "sfAuthenticator": "SNOWFLAKE_JWT",
    "pem_private_key": key_b64,
}

df = (spark.read
      .format("snowflake")
      .options(**sfOptions)
      .option("query", "select current_user(), current_role(), current_database(), current_schema()")
      .load())
df.show()





from base64 import b64encode
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.backends import default_backend

key_path = "/dbfs/FileStore/keys/rsa_key.p8"
passphrase = "987654321"   # replace with your actual passphrase

def get_private_key():
    # Load the encrypted PEM and decode to unencrypted PKCS#8 DER bytes
    with open(key_path, "rb") as key_file:
        p_key = serialization.load_pem_private_key(
            key_file.read(),
            password=passphrase.encode(),
            backend=default_backend()
        )

    # Convert to unencrypted PKCS#8 DER (binary)
    private_bytes = p_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption()
    )

    # Base64-encode it (this is what Snowflake connector expects)
    return b64encode(private_bytes).decode("utf-8")

# ---- Snowflake connection options ----
sfOptions = {
    "sfURL": "uhgdwaas.east-us-2.azure.snowflakecomputing.com",
    "sfUser": "BEERAVALLI_REDDY@OPTUM.COM",
    "sfRole": "AZU_SDRP_LUTF_STG_DEVELOPER_ROLE",
    "sfWarehouse": "LUTF_STG_EVISOR_WH",
    "sfDatabase": "LUTF_STG_EVISOR_DB",
    "sfSchema": "ETL",
    "authenticator": "SNOWFLAKE_JWT",
    "pem_private_key": get_private_key()
}

# ---- Test query ----
df = (spark.read
      .format("snowflake")
      .options(**sfOptions)
      .option("query", "SELECT CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_SCHEMA()")
      .load())

df.show()
