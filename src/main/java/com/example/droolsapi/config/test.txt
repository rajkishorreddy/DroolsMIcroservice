%scala
import org.kie.api.KieServices
import org.kie.api.builder.{KieBuilder, Message, Results}
import org.kie.api.runtime.StatelessKieSession
import org.kie.internal.io.ResourceFactory
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

// ---------------------------------------------------------------------
// 1️⃣  Load the DRL text from DBFS
// ---------------------------------------------------------------------
val path = "/dbfs/FileStore/rules/sample.drl"
val drlBody = scala.io.Source.fromFile(new java.io.File(path), "UTF-8").mkString

// ---------------------------------------------------------------------
// 2️⃣  Function to compile a Drools session (with verbose errors)
// ---------------------------------------------------------------------
def buildSessionVerbose(drl: String): StatelessKieSession = {
  val ks  = KieServices.Factory.get
  val kfs = ks.newKieFileSystem()
  kfs.write(
    ResourceFactory
      .newByteArrayResource(drl.getBytes("UTF-8"))
      .setSourcePath("rules/sample.drl")
  )

  val kb: KieBuilder = ks.newKieBuilder(kfs).buildAll()
  val res: Results = kb.getResults
  if (res.hasMessages(Message.Level.ERROR)) {
    println("==== DROOLS BUILD ERRORS ====")
    res.getMessages(Message.Level.ERROR).forEach(m => println("  " + m))
    throw new IllegalStateException("❌ DRL failed to compile")
  }
  ks.newKieContainer(ks.getRepository.getDefaultReleaseId)
    .getKieBase
    .newStatelessKieSession()
}

// ---------------------------------------------------------------------
// 3️⃣  Cache the compiled session per executor (prevents rebuild each partition)
// ---------------------------------------------------------------------
object DroolsHolder extends Serializable {
  @transient private var cachedSession: StatelessKieSession = _

  def getSession(drl: String): StatelessKieSession = {
    if (cachedSession == null) {
      println(s"Building Drools session on executor ${java.net.InetAddress.getLocalHost.getHostName}")
      cachedSession = buildSessionVerbose(drl)
    }
    cachedSession
  }
}

// ---------------------------------------------------------------------
// 4️⃣  Define schemas and sample input
// ---------------------------------------------------------------------
case class InputRow(indvId: Long, amount: Double)
case class OutputRow(indvId: Long, ruleNum: String, ruleFlag: String, isActive: Boolean)

import spark.implicits._
val df = Seq(
  InputRow(101, 500.0),     // should NOT fire
  InputRow(102, 1200.5),    // should fire
  InputRow(103, 50.0)       // should NOT fire
).toDF

// broadcast rule text to executors
val drlBC = spark.sparkContext.broadcast(drlBody)

// ---------------------------------------------------------------------
// 5️⃣  Execute rules per partition (build once per executor)
// ---------------------------------------------------------------------
val out = df.mapPartitions { it =>
  val session = DroolsHolder.getSession(drlBC.value)

  it.map { r =>
    val fact = new java.util.HashMap[String, Object]()
    fact.put("indvId", r.getAs[Long]("indvId").asInstanceOf[java.lang.Long])
    fact.put("amount", r.getAs[Double]("amount").asInstanceOf[java.lang.Double])

    val result = new java.util.HashMap[String, Object]()
    session.setGlobal("result", result)

    val facts = new java.util.ArrayList[Object]()
    facts.add(fact); facts.add(result)

    try {
      session.execute(facts)
    } catch {
      case e: Throwable =>
        throw new RuntimeException(s"❌ Rule execution failed for fact=$fact\n${e.toString}", e)
    }

    val ruleNum  = Option(result.get("ruleNum")).map(_.toString).getOrElse("NA")
    val ruleFlag = Option(result.get("ruleFlag")).map(_.toString).getOrElse("N")
    val isActive = Option(result.get("isActive")).exists(_.toString.equalsIgnoreCase("true"))

    OutputRow(r.getAs[Long]("indvId"), ruleNum, ruleFlag, isActive)
  }
}.toDF

// ---------------------------------------------------------------------
// 6️⃣  Trigger and show output
// ---------------------------------------------------------------------
out.show(false)
