%scala

// ==================================================
// 0. Imports
// ==================================================
import org.apache.spark.sql.{Row, DataFrame, Dataset, Encoders}
import org.apache.spark.sql.functions._
import scala.collection.JavaConverters._
import scala.util.Try

import org.kie.api.KieServices
import org.kie.api.builder.{KieBuilder, Message}
import org.kie.api.io.ResourceType
import org.kie.api.runtime.StatelessKieSession
import org.kie.internal.io.ResourceFactory

// ==================================================
// 1. SNOWFLAKE CONNECTION OPTIONS
//    (reuse your working values + base64 private key file in DBFS)
// ==================================================
val pemB64: String =
  dbutils.fs.head("dbfs:/FileStore/keys/sf_pk_base64.txt", 1024 * 1024)

val sfOptions = Map(
  "sfURL"          -> "uhgdwaas.east-us-2.azure.snowflakecomputing.com",
  "sfUser"         -> "BEERAVALLI_REDDY@OPTUM.COM",
  "sfRole"         -> "AZU_SDRP_LUTF_STG_DEVELOPER_ROLE",
  "sfWarehouse"    -> "LUTF_STG_EVISOR_WH",
  "sfDatabase"     -> "LUTF_STG_EVISOR_DB",
  "sfSchema"       -> "ETL",
  "authenticator"  -> "SNOWFLAKE_JWT",
  "pem_private_key"-> pemB64
)

// ==================================================
// 2. READ SOURCE DATA FROM SNOWFLAKE
//    NOTE: keep LIMIT now for safety; remove / bump later
// ==================================================
val srcDF: DataFrame =
  spark.read
    .format("snowflake")
    .options(sfOptions)
    .option("query",
    """
      SELECT *
      FROM ELEMENT_DATA_TABLE_KIS
      LIMIT 1000
    """)
    .load()

println(s"âœ… Loaded rows from Snowflake: ${srcDF.count()}")

// ==================================================
// 3. STRONGLY-TYPED EXTRACTORS FROM SPARK ROW
//    We convert Spark Row -> exact Java types that your CommonDataModel ctor needs
//    Handles nulls + multiple numeric representations
// ==================================================

// Integer
def gInt(r: Row, name: String): java.lang.Integer = {
  val v = r.getAs[Any](name)
  v match {
    case null                          => null
    case n: java.lang.Integer          => n
    case n: java.lang.Long             => java.lang.Integer.valueOf(n.intValue)
    case n: java.lang.Short            => java.lang.Integer.valueOf(n.intValue)
    case n: java.lang.Byte             => java.lang.Integer.valueOf(n.intValue)
    case n: java.math.BigDecimal       => java.lang.Integer.valueOf(n.intValue)
    case s: String if s.trim.nonEmpty  =>
      java.lang.Integer.valueOf(s.trim.toInt)
    case _                             => null
  }
}

// Long
def gLong(r: Row, name: String): java.lang.Long = {
  val v = r.getAs[Any](name)
  v match {
    case null                          => null
    case n: java.lang.Long             => n
    case n: java.lang.Integer          => java.lang.Long.valueOf(n.longValue)
    case n: java.math.BigDecimal       => java.lang.Long.valueOf(n.longValue)
    case s: String if s.trim.nonEmpty  =>
      java.lang.Long.valueOf(s.trim.toLong)
    case _                             => null
  }
}

// Double
def gDbl(r: Row, name: String): java.lang.Double = {
  val v = r.getAs[Any](name)
  v match {
    case null                          => null
    case n: java.lang.Double           => n
    case n: java.lang.Float            => java.lang.Double.valueOf(n.toDouble)
    case n: java.math.BigDecimal       => java.lang.Double.valueOf(n.doubleValue)
    case n: java.lang.Long             => java.lang.Double.valueOf(n.toDouble)
    case n: java.lang.Integer          => java.lang.Double.valueOf(n.toDouble)
    case s: String if s.trim.nonEmpty  =>
      java.lang.Double.valueOf(s.trim.toDouble)
    case _                             => null
  }
}

// Float
def gFlt(r: Row, name: String): java.lang.Float = {
  val v = r.getAs[Any](name)
  v match {
    case null                          => null
    case n: java.lang.Float            => n
    case n: java.lang.Double           => java.lang.Float.valueOf(n.toFloat)
    case n: java.math.BigDecimal       => java.lang.Float.valueOf(n.floatValue)
    case n: java.lang.Long             => java.lang.Float.valueOf(n.toFloat)
    case n: java.lang.Integer          => java.lang.Float.valueOf(n.toFloat)
    case s: String if s.trim.nonEmpty  =>
      java.lang.Float.valueOf(s.trim.toFloat)
    case _                             => null
  }
}

// String
def gStr(r: Row, name: String): String = {
  val v = r.getAs[Any](name)
  if (v == null) null else v.toString
}

// Date -> java.sql.Date
def gDate(r: Row, name: String): java.sql.Date = {
  val v = r.getAs[Any](name)
  v match {
    case null                          => null
    case d: java.sql.Date              => d
    case d: java.time.LocalDate        => java.sql.Date.valueOf(d)
    case s: String if s.trim.nonEmpty  =>
      // try yyyy-MM-dd
      Try(java.sql.Date.valueOf(s.trim)).getOrElse(null)
    case _                             => null
  }
}

// ==================================================
// 4. ROW -> CommonDataModel
//    This EXACT order must match your full-args constructor in CommonDataModel
// ==================================================
def toModel(r: Row): CommonDataObject.CommonDataModel =
  new CommonDataObject.CommonDataModel(
    gInt (r,"INDV_ID"),
    gInt (r,"ELEM_NBR"),
    gDbl (r,"ELEM_QTY"),
    gInt (r,"AGE"),
    gStr (r,"MEM_NAME"),
    gDate(r,"ELEM_DT_1"),
    gDate(r,"ELEM_DT_2"),
    gStr (r,"SCOR_TYP_CD"),
    gFlt (r,"SCOR_VAL"),
    gInt (r,"PRG_SRVC_ID"),
    gDate(r,"TRM_DT"),
    gDate(r,"EFF_DT"),
    gStr (r,"ST_ABBR_CD"),
    gStr (r,"OPTUM_SEG_ID"),
    gDate(r,"ID_RUN_DT"),
    gInt (r,"OUTBOUND_FILE_ID"),
    gStr (r,"SCEN_RULE_CD"),
    gDate(r,"DOB"),
    gDate(r,"EFFECTIVE_DATE"),
    gDate(r,"END_DATE"),
    gStr (r,"MED_DX_OF_INTEREST"),
    gStr (r,"SCENARIO_CD"),
    gDate(r,"NEXT_EFFECTIVE_DATE"),
    gInt (r,"GRACE_PERIOD"),
    gDate(r,"MIN_EFF_DATE"),
    gDate(r,"MAX_END_DATE"),
    gDate(r,"FINAL_END_DATE"),
    gStr (r,"ADT_CHIEF_COMPLAINT"),
    gStr (r,"ELEM_SUP"),
    gStr (r,"CONTINUITY"),
    gFlt (r,"MED_COV"),
    gFlt (r,"REL_IPTNT_RISK_12_MO_NBR")
  )

// quick sanity check: try first row
val sanityFirst = srcDF.limit(1).collect()(0)
val sanityModel = toModel(sanityFirst)
println(s"ðŸ§ª Sanity => Built CommonDataModel for INDV_ID=${sanityModel.getIndv_id()}\n")

// ==================================================
// 5. LOAD RULE BUNDLE FROM DBFS AND BROADCAST
//    We DO NOT broadcast a Drools session (not serializable).
//    We broadcast (filePath, drlText). Each executor will build its own session.
// ==================================================

// simple container
case class RuleBundle(drlFiles: Seq[(String,String)]) // (path, bodyText)

def loadRuleBundle(dir: String): RuleBundle = {
  val drls = dbutils.fs.ls(dir)
    .filter(_.name.toLowerCase.endsWith(".drl"))
    .map { f =>
      val body = dbutils.fs.head(f.path)
      (f.path, body)
    }
  RuleBundle(drls)
}

// load DRLs from the folder you already created in DBFS
val ruleBundleLocal = loadRuleBundle("dbfs:/FileStore/rules")
println(s"ðŸ“„ Loaded ${ruleBundleLocal.drlFiles.size} DRL files from DBFS")
ruleBundleLocal.drlFiles.foreach { case (p,_) => println(s"   - $p") }

// broadcast so executors get the raw DRL text
val ruleBundleBC = spark.sparkContext.broadcast(ruleBundleLocal)

// ==================================================
// 6. BUILD A STATELESS KIE SESSION *ON DEMAND* (PER EXECUTOR)
//    Using ONLY the broadcasted drl text
// ==================================================
def buildSessionFromBundle(rb: RuleBundle): StatelessKieSession = {
  val ks: KieServices = KieServices.Factory.get
  val kfs = ks.newKieFileSystem()

  // add each DRL body into KieFileSystem
  rb.drlFiles.foreach { case (path, body) =>
    kfs.write(
      path,
      ResourceFactory
        .newByteArrayResource(body.getBytes("UTF-8"))
        .setResourceType(ResourceType.DRL)
    )
  }

  val kbuilder: KieBuilder = ks.newKieBuilder(kfs).buildAll()
  val results = kbuilder.getResults
  if (results.hasMessages(Message.Level.ERROR)) {
    val errs = results.getMessages(Message.Level.ERROR).asScala.mkString("\n")
    throw new IllegalStateException(s"DROOLS BUILD ERRORS:\n$errs")
  }

  val kbase  = ks
    .newKieContainer(ks.getRepository.getDefaultReleaseId)
    .getKieBase

  val sess   = kbase.newStatelessKieSession()

  // IMPORTANT: we still create a global "result" map (like before)
  val globalResult = new java.util.HashMap[String, Object]()
  sess.setGlobal("result", globalResult)

  sess
}

// ==================================================
// 7. EXECUTE RULES FOR ONE MEMBER (ONE INDV_ID)
//    CHANGES HERE: we now pull CommonDataResultSet from Drools
//    and map it to OutRow that matches your Snowflake table schema.
// ==================================================

// Row shape for final results table
case class OutRow(
  INDV_ID: Long,
  RULE_NUM: String,
  RULE_FLAG: Boolean,
  IS_ACTIVE: String
)

def runRulesForOneMember(
  indvId: Long,
  facts: Seq[CommonDataObject.CommonDataModel]
): Seq[OutRow] = {

  // 1. Build fresh session locally (on executor)
  val sess: StatelessKieSession = buildSessionFromBundle(ruleBundleBC.value)

  // 2. Clear (reset) the global result map for this run
  val resultMap =
    sess.getGlobals.get("result").asInstanceOf[java.util.HashMap[String,Object]]
  resultMap.clear()

  // 3. Insert member facts into Drools and fire rules
  //    NOTE: .asJava to feed the Seq to Drools
  sess.execute(facts.asJava)

  // 4. Collect all CommonDataResultSet that were inserted by the rules
  //    We'll do a second execute() with a ClassObjectFilter, like we did in notebook tests
  val captured = new java.util.ArrayList[CommonDataObject.CommonDataResultSet]()
  sess.execute(
    new java.util.ArrayList[Object](), // empty execution just to query
    new org.kie.api.runtime.ClassObjectFilter(
      classOf[CommonDataObject.CommonDataResultSet]
    )
  )

  // Problem: we populated `captured`? Drools puts matches in that list.
  // Some Drools versions don't auto-fill the list unless you pass the list itself.
  // We'll do that now:
  captured.clear()
  sess.execute(
    captured.asInstanceOf[java.util.Collection[Object]],
    new org.kie.api.runtime.ClassObjectFilter(
      classOf[CommonDataObject.CommonDataResultSet]
    )
  )

  // 5. Map each captured result fact -> OutRow
  val outRows: Seq[OutRow] =
    captured.asScala.toSeq.map { r =>
      val ruleNum   = Option(r.getStatus).getOrElse("NA")
      // You said IS_ACTIVE should be "Y" and RULE_FLAG should come from ruleFlag=true
      OutRow(
        INDV_ID   = indvId,
        RULE_NUM  = ruleNum,
        RULE_FLAG = true,
        IS_ACTIVE = "Y"
      )
    }

  // 6. Return all rows for this INDV_ID
  outRows
}

// ==================================================
// 8. PARTITION FACTS BY INDV_ID, FIRE RULES, COLLECT RESULTS
//    (a) convert DF -> Dataset[CommonDataModel]
//    (b) groupBy INDV_ID
//    (c) run runRulesForOneMember for each INDV_ID group
//    (d) union everything back
// ==================================================

// 8a. convert srcDF rows -> CommonDataModel objects
val factEncoder = Encoders.kryo[CommonDataObject.CommonDataModel]
val factsDS: Dataset[CommonDataObject.CommonDataModel] =
  srcDF.map(row => toModel(row))(factEncoder)

// 8b. group by INDV_ID safely as Long key
def safeIndvId(m: CommonDataObject.CommonDataModel): Long =
  Option(m.getIndv_id())
    .map(_.longValue())
    .getOrElse(-1L)

// group into Dataset[Seq[OutRow]]
val grouped: Dataset[Seq[OutRow]] =
  factsDS
    .groupByKey(m => safeIndvId(m))(Encoders.scalaLong)
    .mapGroups { case (indvId, iterModels) =>
      val batch = iterModels.toSeq
      runRulesForOneMember(indvId, batch)
    }(Encoders.kryo[Seq[OutRow]])

// flatten to Dataset[OutRow]
val allResultsDS: Dataset[OutRow] =
  grouped.flatMap(rows => rows)(Encoders.product[OutRow])

println(s"ðŸ”¥ Collected rule results rows: ${allResultsDS.count()}")
allResultsDS.show(20, truncate=false)

// 8c. turn into DataFrame and add AUDIT_CREAT_DT for Snowflake target table
val resultsDF: DataFrame =
  allResultsDS.toDF()
    .withColumn("AUDIT_CREAT_DT", current_timestamp())

println(s"âœ… Final result rows = ${resultsDF.count()}")
resultsDF.show(20, truncate=false)

// ==================================================
// 9. (OPTIONAL) WRITE BACK TO SNOWFLAKE
//    Uncomment this block when you want to persist results
//    into STREAMLIT_RESULTS_KIS in Snowflake.
// ==================================================
/*
resultsDF
  .write
  .format("snowflake")
  .options(sfOptions)
  .option("dbtable", "STREAMLIT_RESULTS_KIS")  // <-- target table
  .mode("append")
  .save()

println("âœ… Results written back to Snowflake!")
*/
