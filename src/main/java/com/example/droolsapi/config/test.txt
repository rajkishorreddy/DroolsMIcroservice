from pyspark.sql import Row
from pyspark import SparkContext
from datetime import date, timedelta

def run_test_partition(iter_rows):
    sc  = SparkContext.getOrCreate()
    jvm = sc._jvm

    CDM    = jvm.CommonDataObject.CommonDataModel
    Config = jvm.MyDroolsConfig
    JDate  = jvm.java.sql.Date

    ks = Config.kieContainer().newKieSession()

    # ---- build the two facts your rule needs ----
    # 1) SCORE fact (mem_name == "INDV_SCOR_FACT", scor_typ_cd == "FUT_RSK_IN", scor_val >= 3.0)
    score = CDM(
        1, 999, 1.0, 35, "INDV_SCOR_FACT",
        None, None,
        "FUT_RSK_IN", 3.5, 123,
        None, None, None,
        "TX", "SEG1",
        None, None,
        "RULECD",
        None, None, None,
        None, "SCN",
        None, 0,
        None, None, None,
        None, None, None, None,
        0.0, 0.0
    )

    # 2) ELEMENT fact (mem_name == "INDV_ELEMENTS_LAB", elem_nbr == 478, elem_dt_1 within [now-133, now-43])
    target_dt = date.today() - timedelta(days=60)  # inside the required window
    elem = CDM(
        1, 478, 1.0, 35, "INDV_ELEMENTS_LAB",
        JDate.valueOf(target_dt.isoformat()),  # elem_dt_1 => correct java.sql.Date
        None,
        None, 0.0, 123,
        None, None, None,
        "TX", "SEG1",
        None, None,
        "RULECD",
        None, None, None,
        None, "SCN",
        None, 0,
        None, None, None,
        None, None, None, None,
        0.0, 0.0
    )

    ks.insert(score)
    ks.insert(elem)
    ks.fireAllRules()

    out = []
    it = ks.getObjects().iterator()
    while it.hasNext():
        obj = it.next()
        if obj.getClass().getName() == "CommonDataObject.CommonDataResultSet":
            out.append(Row(status=obj.getStatus()))
    ks.dispose()
    return iter(out)

test_rdd = spark.sparkContext.parallelize([1], 1).mapPartitions(run_test_partition)
df = spark.createDataFrame(test_rdd)
df.show(truncate=False)
