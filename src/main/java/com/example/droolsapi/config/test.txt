%scala
import org.apache.spark.sql.{Row, DataFrame, Dataset, Encoder, Encoders}
import org.apache.spark.sql.functions._
import scala.collection.JavaConverters._
import scala.util.Try

// Drools imports
import org.kie.api.KieServices
import org.kie.api.builder.{KieBuilder, Message, Results}
import org.kie.internal.io.ResourceFactory
import org.kie.api.runtime.{StatelessKieSession, ClassObjectFilter}

// Your model classes from the JAR
import CommonDataObject.{CommonDataModel, CommonDataResultSet}

// Spark implicits for encoders
import spark.implicits._

// ======================================================
// 0. Snowflake connection options (same as before)
//    We assume sf_pk_base64.txt already exists and works
// ======================================================
val pemB64: String =
  dbutils.fs.head("dbfs:/FileStore/keys/sf_pk_base64.txt", 1024 * 1024)

val sfOptions = Map(
  "sfURL"           -> "uhgdwaas.east-us-2.azure.snowflakecomputing.com",
  "sfUser"          -> "BEERAVALLI_REDDY@OPTUM.COM",
  "sfRole"          -> "AZU_SDRP_LUTF_STG_DEVELOPER_ROLE",
  "sfWarehouse"     -> "LUTF_STG_EVISOR_WH",
  "sfDatabase"      -> "LUTF_STG_EVISOR_DB",
  "sfSchema"        -> "ETL",
  "authenticator"   -> "SNOWFLAKE_JWT",
  "pem_private_key" -> pemB64
)

// ======================================================
// 1. Pull the source data from Snowflake
//    DEV MODE: LIMIT 1000
//    PROD MODE: remove the LIMIT
// ======================================================
val srcDF: DataFrame =
  spark.read
    .format("snowflake")
    .options(sfOptions)
    .option(
      "query",
      """SELECT *
         FROM ELEMENT_DATA_TABLE_KIS
         LIMIT 1000""" // <-- remove this LIMIT when you're ready for 8.5B (after QA)
    )
    .load()

println(s"✅ Loaded rows from Snowflake: ${srcDF.count()}")

// We'll work with a typed Dataset[Row] downstream
val srcDS: Dataset[Row] = srcDF.as[Row]

// ======================================================
// 2. Strongly-typed getters: convert Spark Row -> Java types
//    These match your CommonDataModel constructor expectations
// ======================================================
def gInt(r: Row, name: String): java.lang.Integer = {
  val v = r.getAs[Any](name)
  v match {
    case null                        => null
    case n: java.lang.Integer        => n
    case n: java.lang.Long           => java.lang.Integer.valueOf(n.intValue)
    case n: java.lang.Short          => java.lang.Integer.valueOf(n.toInt)
    case n: java.lang.Byte           => java.lang.Integer.valueOf(n.toInt)
    case n: java.math.BigDecimal     => java.lang.Integer.valueOf(n.intValue)
    case s: String if s.trim.nonEmpty =>
      java.lang.Integer.valueOf(s.trim.toInt)
    case _                           => null
  }
}

def gLong(r: Row, name: String): java.lang.Long = {
  val v = r.getAs[Any](name)
  v match {
    case null                        => null
    case n: java.lang.Long           => n
    case n: java.lang.Integer        => java.lang.Long.valueOf(n.longValue)
    case n: java.math.BigDecimal     => java.lang.Long.valueOf(n.longValue)
    case s: String if s.trim.nonEmpty =>
      java.lang.Long.valueOf(s.trim.toLong)
    case _                           => null
  }
}

def gDbl(r: Row, name: String): java.lang.Double = {
  val v = r.getAs[Any](name)
  v match {
    case null                        => null
    case n: java.lang.Double         => n
    case n: java.lang.Float          => java.lang.Double.valueOf(n.toDouble)
    case n: java.math.BigDecimal     => java.lang.Double.valueOf(n.doubleValue)
    case n: java.lang.Long           => java.lang.Double.valueOf(n.toDouble)
    case n: java.lang.Integer        => java.lang.Double.valueOf(n.toDouble)
    case s: String if s.trim.nonEmpty =>
      java.lang.Double.valueOf(s.trim.toDouble)
    case _                           => null
  }
}

def gFlt(r: Row, name: String): java.lang.Float = {
  val v = r.getAs[Any](name)
  v match {
    case null                        => null
    case n: java.lang.Float          => n
    case n: java.lang.Double         => java.lang.Float.valueOf(n.toFloat)
    case n: java.math.BigDecimal     => java.lang.Float.valueOf(n.floatValue)
    case n: java.lang.Long           => java.lang.Float.valueOf(n.toFloat)
    case n: java.lang.Integer        => java.lang.Float.valueOf(n.toFloat)
    case s: String if s.trim.nonEmpty =>
      java.lang.Float.valueOf(s.trim.toFloat)
    case _                           => null
  }
}

def gStr(r: Row, name: String): String = {
  val v = r.getAs[Any](name)
  if (v == null) null else v.toString
}

// Spark gives DateType as java.sql.Date normally; also handle LocalDate/String
def gDate(r: Row, name: String): java.sql.Date = {
  val v = r.getAs[Any](name)
  v match {
    case null                        => null
    case d: java.sql.Date           => d
    case d: java.time.LocalDate     => java.sql.Date.valueOf(d)
    case s: String if s.trim.nonEmpty =>
      try { java.sql.Date.valueOf(s.trim) } catch { case _: Throwable => null }
    case _                          => null
  }
}

// ======================================================
// 3. Row -> CommonDataModel
//    MUST match your full constructor order exactly
// ======================================================
def rowToModel(r: Row): CommonDataModel =
  new CommonDataModel(
    gInt(r,"INDV_ID"),
    gInt(r,"ELEM_NBR"),
    gDbl(r,"ELEM_QTY"),
    gInt(r,"AGE"),
    gStr(r,"MEM_NAME"),
    gDate(r,"ELEM_DT_1"),
    gDate(r,"ELEM_DT_2"),
    gStr(r,"SCOR_TYP_CD"),
    gFlt(r,"SCOR_VAL"),
    gInt(r,"PRG_SRVC_ID"),
    gDate(r,"TRM_DT"),
    gDate(r,"EFF_DT"),
    gStr(r,"ST_ABBR_CD"),
    gStr(r,"OPTUM_SEG_ID"),
    gDate(r,"ID_RUN_DT"),
    gInt(r,"OUTBOUND_FILE_ID"),
    gStr(r,"SCEN_RULE_CD"),
    gDate(r,"DOB"),
    gDate(r,"EFFECTIVE_DATE"),
    gDate(r,"END_DATE"),
    gStr(r,"MED_DX_OF_INTEREST"),
    gStr(r,"SCENARIO_CD"),
    gDate(r,"NEXT_EFFECTIVE_DATE"),
    gInt(r,"GRACE_PERIOD"),
    gDate(r,"MIN_EFF_DATE"),
    gDate(r,"MAX_END_DATE"),
    gDate(r,"FINAL_END_DATE"),
    gStr(r,"ADT_CHIEF_COMPLAINT"),
    gStr(r,"ELEM_SUP"),
    gStr(r,"CONTINUITY"),
    gFlt(r,"MED_COV"),
    gFlt(r,"REL_IPTNT_RISK_12_MO_NBR")
  )

// helper to turn INDV_ID into a non-null Long key for grouping
def indvKey(r: Row): Long = {
  val raw = r.getAs[Any]("INDV_ID")
  raw match {
    case null                    => -1L
    case n: java.lang.Integer    => n.longValue()
    case n: java.lang.Long       => n.longValue()
    case n: java.math.BigDecimal => n.longValue()
    case s: String if s.trim.nonEmpty =>
      Try(s.trim.toLong).getOrElse(-1L)
    case _ =>
      -1L
  }
}

// ======================================================
// 4. Pre-load DRL text on the DRIVER
//    We'll broadcast it because executors CANNOT call dbutils.fs
// ======================================================
val drlFiles: Seq[(String,String)] =
  dbutils.fs.ls("dbfs:/FileStore/rules")
    .filter(_.name.endsWith(".drl"))
    .map { f =>
      val body = dbutils.fs.head(f.path)
      (f.path, body)
    }

println(s"✅ Found DRLs: ${drlFiles.map(_._1).mkString(", ")}")

val drlBC = spark.sparkContext.broadcast(drlFiles)

// ======================================================
// 5. Helper that executors can call to build a Drools session
//    from those broadcast DRL strings
// ======================================================
def makeSessionFromDrls(drls: Seq[(String,String)]): StatelessKieSession = {
  val ks  = KieServices.Factory.get
  val kfs = ks.newKieFileSystem()

  drls.foreach { case (path, body) =>
    kfs.write(
      ResourceFactory
        .newByteArrayResource(body.getBytes("UTF-8"))
        .setSourcePath(path)
    )
  }

  val kb: KieBuilder = ks.newKieBuilder(kfs).buildAll()
  val res: Results = kb.getResults
  if (res.hasMessages(Message.Level.ERROR)) {
    // if something is wrong with DRL we fail fast
    val msgs = res.getMessages(Message.Level.ERROR)
      .asScala
      .map(_.toString)
      .mkString("\n  ")
    throw new IllegalStateException(s"Drools compile error:\n  ${msgs}")
  }

  ks.newKieContainer(ks.getRepository.getDefaultReleaseId)
    .getKieBase
    .newStatelessKieSession()
}

// ======================================================
// 6. Result row case class (what we'll collect and write out)
//    Add whatever fields you want from the global "result" map
// ======================================================
case class OutRow(
  INDV_ID: Long,
  RULE_NUM: String,
  RULE_FLAG: Boolean,
  IS_ACTIVE: String
)

// ======================================================
// 7. Fire rules for ONE MEMBER (one INDV_ID group) on EXECUTOR
//    - build session locally
//    - set global result map
//    - insert all that member's CommonDataModel facts
//    - run Drools
//    - read global map -> OutRow
//
// NOTE: we keep global HashMap pattern because that's how
// your DRLs currently communicate results.
// Later, if you start inserting CommonDataResultSet objects and want
// to pull them directly, we can extend this.
// ======================================================
def runRulesForOneMember(
  indvId: Long,
  rowsIter: Iterator[Row],
  drlTexts: Seq[(String,String)]
): Iterator[OutRow] = {

  // build facts for just this INDV_ID
  val facts: Seq[CommonDataModel] = rowsIter.toSeq.map(rowToModel)

  if (facts.isEmpty) {
    Iterator.empty
  } else {
    val sess  = makeSessionFromDrls(drlTexts)
    val gmap  = new java.util.HashMap[String,Object]()
    sess.setGlobal("result", gmap)

    // insert member facts
    val javaFacts: java.util.List[Object] =
      facts.map(_.asInstanceOf[Object]).asJava
    sess.execute(javaFacts)

    // ask for result facts (and/or let globals populate)
    val emptyList = new java.util.ArrayList[Object]()
    sess.execute(
      emptyList,
      new ClassObjectFilter(classOf[CommonDataResultSet])
    )

    // Build OutRow
    val out = OutRow(
      INDV_ID   = indvId,
      RULE_NUM  = Option(gmap.get("ruleNum")).map(_.toString).getOrElse("NA"),
      RULE_FLAG = Option(gmap.get("ruleFlag"))
                    .exists(_.toString.equalsIgnoreCase("true")),
      IS_ACTIVE = Option(gmap.get("isActive")).map(_.toString).getOrElse("Y")
    )

    Iterator(out)
  }
}

// ======================================================
// 8. DISTRIBUTED EXECUTION 👇
//    group by INDV_ID and run Drools per group IN PARALLEL
//
//    - repartition on INDV_ID so each member tends to be in one partition
//    - groupByKey to get KeyValueGroupedDataset[Long, Row]
//    - flatMapGroups runs once per INDV_ID per partition
// ======================================================
val resultsDS: Dataset[OutRow] =
  srcDS
    .repartition(col("INDV_ID"))               // good shuffle key
    .groupByKey(row => indvKey(row))(Encoders.scalaLong)
    .flatMapGroups { case (indvId, rowsIter) =>
      // each executor has access to drlBC.value (Seq[(path,body)])
      runRulesForOneMember(indvId, rowsIter, drlBC.value)
    }(Encoders.product[OutRow]) // Spark encoder for case class

// ======================================================
// 9. MATERIALIZE RESULTS
// ======================================================
val resultsDF = resultsDS
  .withColumn("AUDIT_CREAT_DT", current_timestamp())

println(s"✅ Final result rows = ${resultsDF.count()}")
resultsDF.show(20, truncate=false)

// ======================================================
// 10. (OPTIONAL) WRITE BACK TO SNOWFLAKE
//     When you're ready, uncomment this block 👇
// ======================================================
/*
resultsDF
  .write
  .format("snowflake")
  .options(sfOptions)
  .option("dbtable", "STREAMLIT_RESULTS_KIS")   // <-- target table
  .mode("append")
  .save()

println("✅ Results written back to Snowflake!")
*/
