// =====================================================
// 6. Insert facts, fire rules, collect + aggregate like Snowflake
// =====================================================
import org.kie.api.runtime.ClassObjectFilter
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule

// insert all facts for this INDV_ID
memberFacts.foreach(sess.insert)

// fire rules
val fired = sess.fireAllRules()
println(s"ðŸ”¥ Fired $fired rules for INDV_ID=$targetIndvId")

// pull EVERYTHING that's not an input CommonDataModel
val allAfterFire_raw: Seq[AnyRef] =
  sess.getObjects(/* no filter = all objects */).asScala.toSeq.map(_.asInstanceOf[AnyRef])

val nonInputFacts: Seq[AnyRef] =
  allAfterFire_raw.filterNot(_.isInstanceOf[CommonDataObject.CommonDataModel])

println(s"ðŸ§ª Session produced ${nonInputFacts.size} non-input facts after rules")

// mirror factsToString() logic from Snowflake
val mapper = {
  val m = new ObjectMapper()
  // register Scala module so case classes serialize cleanly too
  m.registerModule(DefaultScalaModule)
  m
}

val maxLen = 5000
val sb     = new StringBuilder()
sb.append("[")

val seen = new java.util.HashSet[String]()
var first = true
var stop  = false

nonInputFacts.foreach { fact =>
  if (!stop) {
    val json = try {
      mapper.writeValueAsString(fact)
    } catch {
      case e: Exception =>
        // mimic legacy "append '' if can't serialize"
        ""
    }

    if (json != "" && seen.add(json)) {
      if (!first) sb.append(", ")
      sb.append(json)
      first = false

      if (sb.length > maxLen) {
        sb.setLength(maxLen)
        sb.append("...")
        stop = true
      }
    }
  }
}

sb.append("]")
val aggregatedJsonString = sb.toString()

println("ðŸ“¦ Aggregated (factsToString-style) output:")
println(aggregatedJsonString)

// build final single-row output just like endPartition() returned
case class FinalRowForSnowflake(
  INDV_ID: Long,
  RULE_RESULTS_JSON: String,
  AUDIT_CREAT_DT: java.sql.Timestamp
)

import java.sql.Timestamp
val nowTs = new Timestamp(System.currentTimeMillis())

val finalRowObj = FinalRowForSnowflake(
  INDV_ID = targetIndvId,
  RULE_RESULTS_JSON = aggregatedJsonString,
  AUDIT_CREAT_DT = nowTs
)

// turn into a DataFrame with ONE ROW for this INDV_ID
val finalDF = spark.createDataFrame(Seq(finalRowObj))

println("ðŸš€ Final DF (1 row per INDV_ID) like Snowflake UDTF:")
finalDF.show(truncate=false)

// cleanup session
sess.dispose()
